import math
import re
import pandas as pd
import streamlit as st
import openai
import plotly.express as px
import plotly.graph_objects as go

openai.api_key = st.secrets["openai"]["api_key"]
client = openai.OpenAI(api_key=openai.api_key)

KDC_KEYWORD_MAP = {
    '000 ì´ë¥˜': ["ë°±ê³¼ì‚¬ì „", "ë„ì„œê´€", "ë…ì„œ", "ë¬¸í—Œì •ë³´", "ê¸°ë¡", "ì¶œíŒ", "ì„œì§€"],
    '100 ì² í•™': ["ì² í•™", "ëª…ìƒ", "ìœ¤ë¦¬", "ë…¼ë¦¬í•™", "ì‹¬ë¦¬í•™"],
    '200 ì¢…êµ': ["ì¢…êµ", "ê¸°ë…êµ", "ë¶ˆêµ", "ì²œì£¼êµ", "ì‹ í™”", "ì‹ ì•™", "ì¢…êµí•™"],
    '300 ì‚¬íšŒê³¼í•™': ["ì‚¬íšŒ", "ì •ì¹˜", "ê²½ì œ", "ë²•ë¥ ", "í–‰ì •", "êµìœ¡", "ë³µì§€", "ì—¬ì„±", "ë…¸ì¸", "ìœ¡ì•„", "ì•„ë™ë³µì§€", "ì‚¬íšŒë¬¸ì œ", "ë…¸ë™", "í™˜ê²½ë¬¸ì œ", "ì¸ê¶Œ"],
    '400 ìì—°ê³¼í•™': ["ìˆ˜í•™", "ë¬¼ë¦¬", "í™”í•™", "ìƒë¬¼", "ì§€êµ¬ê³¼í•™", "ê³¼í•™", "ì²œë¬¸", "ê¸°í›„", "ì˜í•™", "ìƒëª…ê³¼í•™"],
    '500 ê¸°ìˆ ê³¼í•™': ["ê±´ê°•", "ì˜ë£Œ", "ìš”ë¦¬", "ê°„í˜¸", "ê³µí•™", "ì»´í“¨í„°", "AI", "IT", "ë†ì—…", "ì¶•ì‚°", "ì‚°ì—…", "ê¸°ìˆ ", "ë¯¸ìš©"],
    '600 ì˜ˆìˆ ': ["ë¯¸ìˆ ", "ìŒì•…", "ë¬´ìš©", "ì‚¬ì§„", "ì˜í™”", "ì—°ê·¹", "ë””ìì¸", "ê³µì˜ˆ", "ì˜ˆìˆ ", "ë¬¸í™”ì˜ˆìˆ "],
    '700 ì–¸ì–´': ["ì–¸ì–´", "êµ­ì–´", "ì˜ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´", "ì™¸êµ­ì–´", "í•œì", "ë¬¸ë²•"],
    '800 ë¬¸í•™': ["ì†Œì„¤", "ì‹œ", "ìˆ˜í•„", "ì—ì„¸ì´", "í¬ê³¡", "ë¬¸í•™", "ë™í™”", "ì›¹íˆ°", "íŒíƒ€ì§€", "ë¬¸ì˜ˆ"],
    '900 ì—­ì‚¬Â·ì§€ë¦¬': ["ì—­ì‚¬", "ì§€ë¦¬", "í•œêµ­ì‚¬", "ì„¸ê³„ì‚¬", "ì—¬í–‰", "ë¬¸í™”ìœ ì‚°", "ê´€ê´‘"],
    'ì›ì„œ(ì˜ì–´)': ["ì›ì„œ", "ì˜ë¬¸ë„ì„œ", "ì˜ë¬¸íŒ", "ì˜ì–´ì›ì„œ"],
    'ì—°ì†ê°„í–‰ë¬¼': ["ì¡ì§€", "ê°„í–‰ë¬¼", "ì—°ì†ê°„í–‰ë¬¼"],
    'í•´ë‹¹ì—†ìŒ': []
}


def is_trivial(text):
    text = str(text).strip()
    return text in ["", "X", "x", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬", "ì—†ìŒ"]


def map_keyword_to_category(keyword):
    for cat, kws in KDC_KEYWORD_MAP.items():
        if any(k in keyword for k in kws):
            return cat
    return "í•´ë‹¹ì—†ìŒ"


def split_keywords_simple(text):
    parts = re.split(r"[.,/\s]+", text)
    return [p.strip() for p in parts if len(p.strip()) > 1]


@st.cache_data(show_spinner=False)
def extract_keyword_and_audience(responses, batch_size=20):
    results = []
    for i in range(0, len(responses), batch_size):
        batch = responses[i:i + batch_size]
        prompt = f"""ë‹¹ì‹ ì€ ë„ì„œê´€ ììœ ì‘ë‹µì—ì„œ ì•„ë˜ í˜•ì‹ì˜ JSON ë°°ì—´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
ê° ê°ì²´ëŠ” ì‘ë‹µ, í‚¤ì›Œë“œ ëª©ë¡(1~3ê°œ), ëŒ€ìƒì¸µ(ìœ ì•„/ì•„ë™/ì²­ì†Œë…„/ì¼ë°˜)ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œOutput:
[
  {{"response": "ì‘ë‹µ1", "keywords": ["í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2"], "audience": "ì²­ì†Œë…„"}},
  ...
]

ì‘ë‹µ ëª©ë¡:
{chr(10).join(f"{j+1}. {txt}" for j, txt in enumerate(batch))}
"""
        resp = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "system", "content": prompt}],
            temperature=0.2,
            max_tokens=300
        )
        content = resp.choices[0].message.content.strip()
        try:
            data = pd.read_json(content)
        except Exception:
            data = []
            for txt in batch:
                kws = split_keywords_simple(txt)
                audience = 'ì¼ë°˜'
                for w in ['ì–´ë¦°ì´','ì´ˆë“±']:
                    if w in txt:
                        audience='ì•„ë™'
                for w in ['ìœ ì•„','ë¯¸ì·¨í•™','ê·¸ë¦¼ì±…']:
                    if w in txt:
                        audience='ìœ ì•„'
                for w in ['ì²­ì†Œë…„','ì§„ë¡œ','ìê¸°ê³„ë°œ']:
                    if w in txt:
                        audience='ì²­ì†Œë…„'
                data.append({
                    'response': txt,
                    'keywords': kws,
                    'audience': audience
                })
            data = pd.DataFrame(data)
        for _, row in data.iterrows():
            results.append((row['response'], row['keywords'], row['audience']))
    return results


@st.cache_data(show_spinner=False)
def process_answers(responses):
    expanded = []
    for ans in responses:
        if is_trivial(ans):
            continue
        parts = [p.strip() for p in ans.split(',') if p.strip()]
        if len(parts) > 1:
            expanded.extend(parts)
        else:
            expanded.append(ans)

    processed = []
    batches = extract_keyword_and_audience(expanded, batch_size=8)
    for resp, kws, aud in batches:
        if is_trivial(resp):
            continue
        if not kws:
            kws = split_keywords_simple(resp)
        for kw in kws:
            cat = map_keyword_to_category(kw)
            if cat == 'í•´ë‹¹ì—†ìŒ' and aud == 'ì¼ë°˜':
                continue
            processed.append({
                'ì‘ë‹µ': resp,
                'í‚¤ì›Œë“œ': kw,
                'ì£¼ì œë²”ì£¼': cat,
                'ëŒ€ìƒë²”ì£¼': aud
            })
    return pd.DataFrame(processed)


def show_short_answer_keyword_analysis(df_result):
    st.subheader("ğŸ“˜ Q9-DS-4 ë‹¨ë¬¸ ì‘ë‹µ í‚¤ì›Œë“œ ë¶„ì„")
    order = list(KDC_KEYWORD_MAP.keys())
    df_cat = df_result.groupby("ì£¼ì œë²”ì£¼")["í‚¤ì›Œë“œ"].count().reindex(order, fill_value=0).reset_index(name="ë¹ˆë„ìˆ˜")
    fig = px.bar(df_cat, x="ì£¼ì œë²”ì£¼", y="ë¹ˆë„ìˆ˜", title="ì£¼ì œë²”ì£¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„", text="ë¹ˆë„ìˆ˜")
    fig.update_traces(textposition="outside")
    st.plotly_chart(fig, use_container_width=True)
    df_aud = df_result.groupby("ëŒ€ìƒë²”ì£¼")["í‚¤ì›Œë“œ"].count().reset_index(name="ë¹ˆë„ìˆ˜")
    fig2 = px.bar(df_aud, x="ëŒ€ìƒë²”ì£¼", y="ë¹ˆë„ìˆ˜", title="ëŒ€ìƒë²”ì£¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„", text="ë¹ˆë„ìˆ˜", color="ëŒ€ìƒë²”ì£¼")
    fig2.update_traces(textposition="outside")
    st.plotly_chart(fig2, use_container_width=True)
    st.markdown("#### ğŸ” ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”")
    st.dataframe(df_result[["ì‘ë‹µ", "í‚¤ì›Œë“œ", "ì£¼ì œë²”ì£¼", "ëŒ€ìƒë²”ì£¼"]])


def page_short_keyword(df):
    with st.spinner("ğŸ” GPT ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘..."):
        target_cols = [col for col in df.columns if "Q9-DS-4" in col]
        if not target_cols:
            st.warning("Q9-DS-4 ê´€ë ¨ ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        answers = df[target_cols[0]].dropna().astype(str).tolist()
        df_result = process_answers(answers)
        show_short_answer_keyword_analysis(df_result)
